{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# This software is provided as-is, without warranty or representation for any use or purpose. \n",
    "# Your use of it is subject to your agreements with Google.‚Äù  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVX0RV_s8zZ7"
   },
   "source": [
    "\n",
    "# <font color='gold'>Vertex end-to-end Demo</font>\n",
    "---\n",
    "## Agenda\n",
    "  1. Environment setup\n",
    "  1. Feature store\n",
    "  1. Explainable AI Model\n",
    "  1. Vertex Pipeline: custom/AutoML models training, evaluation, and deployment\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "---\n",
    "---\n",
    "## 1. Environment Setup </font>\n",
    "---\n",
    "### 1.1 Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "---\n",
    "### 1.2 Install additional packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IaYsrh0Tc17L"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    USER_FLAG = \"\"\n",
    "else:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxtzwPPNZ-SH"
   },
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp==1.6.6 google-cloud-pipeline-components==0.1.1 --upgrade\n",
    "!pip3 install install Tensorflow --user\n",
    "!pip3 install explainable-ai-sdk --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "---\n",
    "### 1.3 Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GPgNN7eeX1l"
   },
   "source": [
    "Check the KFP SDK version. It should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NN0mULkEeb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.6.6\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT4WAH38l5et"
   },
   "source": [
    "---\n",
    "### 1.4 Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI, Cloud Storage, and Compute Engine APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com). \n",
    "\n",
    "1. Follow the \"**Configuring your project**\" instructions from the Vertex Pipelines documentation.\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PzU2mpv4LG4o",
    "outputId": "b348e599-fc8d-4b5e-e71a-6be5fdea7c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project erwinh-ml-demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "outputId": "ebcd6d08-940f-4791-bef8-fd5b980f4037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  erwinh-ml-demos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"erwinh-ml-demos\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"erwinh-ml-demos\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "---\n",
    "### 1.5 Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxhCPW6e46EF"
   },
   "source": [
    "---\n",
    "### 1.6 Create a Cloud Storage bucket as necessary\n",
    "\n",
    "You will need a Cloud Storage bucket for this example.  If you don't have one that you want to use, you can make one now.\n",
    "\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://erwin-demo-vertex\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://erwin-demo-vertex/data/\n",
      "                                 gs://erwin-demo-vertex/output-marketing-model/\n",
      "                                 gs://erwin-demo-vertex/staging/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "---\n",
    "### 1.7 Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYtGjGG45ELJ"
   },
   "source": [
    "Define some constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5zmD19ryCre7",
    "outputId": "cb32c4f7-618c-4ecc-8c41-03726546b9f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n",
      "gs://erwin-demo-vertex/pipeline_root/erwinh/20220318103505 train_deploy20220318103505\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "USER = \"erwinh\"  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)\n",
    "\n",
    "WORKING_DIR = f\"{PIPELINE_ROOT}/{TIMESTAMP}\"\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"train_deploy{TIMESTAMP}\"\n",
    "print(WORKING_DIR, MODEL_DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IprQaSI25oSk"
   },
   "source": [
    "Do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UFDUBveR5UfJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 05:53:33.408887: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import kfp\n",
    "import google.auth\n",
    "import explainable_ai_sdk\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "from google.cloud.aiplatform import datasets\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from google.cloud.aiplatform_v1beta1 import (\n",
    "    FeaturestoreOnlineServingServiceClient, FeaturestoreServiceClient)\n",
    "from google.cloud.aiplatform_v1beta1.types import FeatureSelector, IdMatcher\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    entity_type as entity_type_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore as featurestore_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_monitoring as featurestore_monitoring_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_online_service as featurestore_online_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_service as featurestore_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2.dsl import ClassificationMetrics, Metrics, Output, Input, component, Model, Dataset, Artifact, Condition\n",
    "\n",
    "from explainable_ai_sdk.metadata.tf.v2 import SavedModelMetadataBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A7SVkscLG40"
   },
   "source": [
    "Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "37SGyuumLG40"
   },
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"us-central1-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM1ajOqWLG40"
   },
   "source": [
    "---\n",
    "---\n",
    "## 2. Feature Store Creation\n",
    "---\n",
    "### 2.1 Prepare training dataset\n",
    "1. Download sample dataset from https://www.kaggle.com/ashishkumarsingh123/telecom-churn-dataset\n",
    "2. Create a new dataset in your GCP BQ instance, and upload the downloaded data to BQ as a new table. e.g.: bq://sample-project.ml_sample.telecom_churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_OZ4CQyLG41",
    "tags": []
   },
   "source": [
    "---\n",
    "### 2.2 Create Feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5Is84K2LG41",
    "outputId": "d92bc04d-6281-44e6-9deb-0aa68a485469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create admin_client for CRUD and data_client for reading feature values.\n",
    "admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n",
    "data_client = FeaturestoreOnlineServingServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")\n",
    "\n",
    "# Represents featurestore resource path.\n",
    "BASE_RESOURCE_PATH = admin_client.common_location_path(PROJECT_ID, REGION)\n",
    "FEATURESTORE_ID_TO_CREATE = \"telecom_churn_prediction_{timestamp}\".format(timestamp=TIMESTAMP)\n",
    "create_lro = admin_client.create_featurestore(\n",
    "    featurestore_service_pb2.CreateFeaturestoreRequest(\n",
    "        parent=BASE_RESOURCE_PATH,\n",
    "        featurestore_id=FEATURESTORE_ID_TO_CREATE,\n",
    "        featurestore=featurestore_pb2.Featurestore(\n",
    "            #display_name=\"Featurestore for telco churn prediction\",\n",
    "            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n",
    "                fixed_node_count=3\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "# Wait for LRO to finish and get the LRO result.\n",
    "print(create_lro.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpRb3KBOLG46"
   },
   "source": [
    "---\n",
    "### 2.2 Create Entity Type\n",
    "You can specify a monitoring config which will by default be inherited by all Features under this EntityType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkyPl-25LG46",
    "outputId": "9f147f8e-4e61-4ac5-8c68-9b45dd5a7fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users\"\n",
      "etag: \"AMEw9yMZsrwIzYcA-OcFn5qWCoPAaAPbeE7BgcaSBzfmCXhgePUU\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create users entity type with monitoring enabled.\n",
    "# All Features belonging to this EntityType will by default inherit the monitoring config.\n",
    "users_entity_type_lro = admin_client.create_entity_type(\n",
    "    featurestore_service_pb2.CreateEntityTypeRequest(\n",
    "        parent=admin_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID_TO_CREATE),\n",
    "        entity_type_id=\"users\",\n",
    "        entity_type=entity_type_pb2.EntityType(\n",
    "            description=\"Users entity\",\n",
    "            monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                    monitoring_interval=Duration(seconds=86400),  # 1 day\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Similarly, wait for EntityType creation operation.\n",
    "print(users_entity_type_lro.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQV6tcD9LG46"
   },
   "source": [
    "\n",
    "---\n",
    "### 2.3 Create Feature Type\n",
    "\n",
    "You can also set a custom monitoring configuration at the Feature level, and view the properties and metrics in the console: sample [properties](https://storage.googleapis.com/cloud-samples-data/ai-platform-unified/datasets/featurestore/Feature%20Properties.png), sample [metrics](https://storage.googleapis.com/cloud-samples-data/ai-platform-unified/datasets/featurestore/Feature%20Snapshot%20Distribution.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b_2pxDgLG4_",
    "outputId": "de7dc4a2-8cbf-4a8e-9587-8d446a48dcf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/mobile_number\"\n",
       "  etag: \"AMEw9yNZPqqfXGtXjxqYK7e05eKVOtW8Vafb9oH_9gPzgtgWn5Rq\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/arpu_m1\"\n",
       "  etag: \"AMEw9yOSWdk52FkHNIfTFTSzpCSEIgGsrpkk51iVA9pmw6BcRGMW\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/arpu_m2\"\n",
       "  etag: \"AMEw9yNyLmu8NYgbzlw39UnjnuEkYcBkF_eqnScS4xF5XCKpHdR4\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/arpu_m3\"\n",
       "  etag: \"AMEw9yMAX78gaTSZMc5qgYbM-hi1z5dz3ge-KYWGSJHuMV8BhR4c\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/arpu_m4\"\n",
       "  etag: \"AMEw9yMiYMJFq0TeWiAEDKV0p5Y_r99c_ZpctSIeJ5XH7u7UVB-b\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/mou_m1\"\n",
       "  etag: \"AMEw9yPx2Iqi1HdayiYwNoPgX8BRulNeJGYg6hOaFKZy0PpGmyKK\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/mou_m2\"\n",
       "  etag: \"AMEw9yNvkqkmNEeNDKlmAsJE7QplImHGRXf4SbGMTA4SNV87u2Wd\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/mou_m3\"\n",
       "  etag: \"AMEw9yOqR0rS223AU1DliVW9seJQucIQSL7E1vApiGX9SvAeJXf3\"\n",
       "}\n",
       "features {\n",
       "  name: \"projects/33856278209/locations/us-central1/featurestores/telecom_churn_prediction_20210927065529/entityTypes/users/features/is_churn\"\n",
       "  etag: \"AMEw9yMuWbYo1dO0SFvVdGfYmIbRbyz3rj1Cb8KX8jdI2Phe8r73\"\n",
       "}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create features for the 'users' entity.\n",
    "admin_client.batch_create_features(\n",
    "    parent=admin_client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID_TO_CREATE, \"users\"),\n",
    "    requests=[\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.INT64,\n",
    "                description=\"mobile_number\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"mobile_number\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
    "                description=\"average revenue per user on first month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"arpu_m1\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
    "                description=\"average revenue per user on second month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"arpu_m2\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
    "                description=\"average revenue per user on third month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"arpu_m3\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
    "                description=\"average revenue per user on forth month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"arpu_m4\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
    "                description=\"Minutes of usage - voice calls on first month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"mou_m1\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
    "                description=\"Minutes of usage - voice calls month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"mou_m2\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
    "                description=\"Minutes of usage - voice calls on third month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"mou_m3\",\n",
    "        ),\n",
    "        featurestore_service_pb2.CreateFeatureRequest(\n",
    "            feature=feature_pb2.Feature(\n",
    "                value_type=feature_pb2.Feature.ValueType.BOOL,\n",
    "                description=\"if the user churn on the forth month. Judged by the spend > 0 on forth month\",\n",
    "                monitoring_config=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                    snapshot_analysis=featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                        disabled=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            feature_id=\"is_churn\",\n",
    "        ),\n",
    "    ],\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFTRRIELLG5B"
   },
   "source": [
    "---\n",
    "### 2.4 Feature Engineering\n",
    "Select a subset of feature from raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jxqvxSqLG5B"
   },
   "outputs": [],
   "source": [
    "BQ_RAW_DATA = \"bq://erwinh-ml-demos.telecom_churn.churn_v1\"  \n",
    "FEATURE_DESTINATION = \"bq://erwinh-ml-demos.ml_sample.feature_to_import_\"+TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZPVmdyTLG5B",
    "outputId": "3afc3915-fe78-411d-fdc9-e5d6312e0ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f9f50098510>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = bigquery.Client('erwinh-ml-demos')\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(destination=FEATURE_DESTINATION.split('/')[-1])\n",
    "\n",
    "sql = \"\"\"\n",
    "    SELECT cast(mobile_number as string) mobile_number,arpu_6,arpu_7,arpu_8,arpu_9<=0 as is_churn,onnet_mou_6,onnet_mou_7,onnet_mou_8,CURRENT_TIMESTAMP() as update_time\n",
    "    FROM `{}`;\n",
    "\"\"\".format(BQ_RAW_DATA.split('/')[-1])\n",
    "\n",
    "# Start the query, passing in the extra configuration.\n",
    "query_job = client.query(sql, job_config=job_config)  # Make an API request.\n",
    "query_job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UglME29MLG5C"
   },
   "source": [
    "---\n",
    "### 2.5 Feature Importing\n",
    "Import processed features into feature store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPeEW3IALG5C",
    "outputId": "16411034-21c1-4251-c523-d6c89c450c07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imported_entity_count: 99999\n",
       "imported_feature_value_count: 686819"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create admin_client for CRUD and data_client for reading feature values.\n",
    "admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n",
    "data_client = FeaturestoreOnlineServingServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")\n",
    "\n",
    "# Represents featurestore resource path.\n",
    "BASE_RESOURCE_PATH = admin_client.common_location_path(PROJECT_ID, REGION)\n",
    "import_users_request = featurestore_service_pb2.ImportFeatureValuesRequest(\n",
    "    entity_type=admin_client.entity_type_path(\n",
    "        PROJECT_ID, REGION, FEATURESTORE_ID_TO_CREATE, \"users\"\n",
    "    ),\n",
    "    bigquery_source=io_pb2.BigQuerySource(\n",
    "        # Source\n",
    "        input_uri=FEATURE_DESTINATION\n",
    "    ),\n",
    "    entity_id_field=\"mobile_number\",\n",
    "    feature_specs=[\n",
    "        # Features\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"arpu_m1\", source_field=\"arpu_6\"),\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"arpu_m2\", source_field=\"arpu_7\"),\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"arpu_m3\", source_field=\"arpu_8\"),\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"is_churn\", source_field=\"is_churn\"),\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"mou_m1\", source_field=\"onnet_mou_6\"),\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"mou_m2\", source_field=\"onnet_mou_7\"),\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"mou_m3\", source_field=\"onnet_mou_8\"),\n",
    "    ],\n",
    "    feature_time_field=\"update_time\",\n",
    "    worker_count=10,\n",
    ")\n",
    "ingestion_lro = admin_client.import_feature_values(import_users_request)\n",
    "ingestion_lro.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUQaQBNmLG5C"
   },
   "source": [
    "---\n",
    "---\n",
    "## 3. Explainable AI Model\n",
    "Train a customized model with TF, and explain it with explainable AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEdN0K2tLG5D"
   },
   "source": [
    "---\n",
    "#### 3.0 Serve training data from feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kE4vXUsLG5E"
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_TABLE = 'erwinh-ml-demos.ml_sample.training_data_'+TIMESTAMP\n",
    "FEATURESTORE_ID = 'telecom_churn_prediction_20210708091932'\n",
    "TRAINING_DATA_SELECTOR_LOC = BUCKET_NAME + '/dataset/query_instance_2.csv'\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "query_string = \"\"\"\n",
    "SELECT\n",
    "    mobile_number\n",
    "FROM `{}`\n",
    "\"\"\".format(BQ_RAW_DATA.split('/')[-1])\n",
    "\n",
    "user_df = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "\n",
    "X_train = user_df['mobile_number']\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "res = pd.DataFrame()\n",
    "res['users']  = X_train\n",
    "res['timestamp'] = current_time\n",
    "res.to_csv(TRAINING_DATA_SELECTOR_LOC, index=False)\n",
    "admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n",
    "batch_serving_request = featurestore_service_pb2.BatchReadFeatureValuesRequest(\n",
    "    # featurestore info\n",
    "    featurestore=admin_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n",
    "    # URL for the label data, i.e., Table 1.\n",
    "    csv_read_instances=io_pb2.CsvSource(\n",
    "        gcs_source=io_pb2.GcsSource(uris=[TRAINING_DATA_SELECTOR_LOC])\n",
    "    ),\n",
    "    destination=featurestore_service_pb2.FeatureValueDestination(\n",
    "        bigquery_destination=io_pb2.BigQueryDestination(\n",
    "            # Output to BigQuery table created earlier\n",
    "            output_uri='bq://'+TRAINING_DATA_TABLE\n",
    "        )\n",
    "    ),\n",
    "    entity_type_specs=[\n",
    "        featurestore_service_pb2.BatchReadFeatureValuesRequest.EntityTypeSpec(\n",
    "            entity_type_id=\"users\",\n",
    "            feature_selector=FeatureSelector(\n",
    "                id_matcher=IdMatcher(\n",
    "                    ids=[\n",
    "                        # features, use \"*\" if you want to select all features within this entity type\n",
    "                        \"mou_m1\",\n",
    "                        \"mou_m2\",\n",
    "                        \"mou_m3\",\n",
    "                        \"arpu_m1\",\n",
    "                        \"arpu_m2\",\n",
    "                        \"arpu_m3\",\n",
    "                        \"is_churn\"\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "batch_serving_lro = admin_client.batch_read_feature_values(batch_serving_request)\n",
    "batch_serving_lro.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKW0SP_HLG5E"
   },
   "source": [
    "---\n",
    "### 3.1 train a customized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwscwRAxLG5F"
   },
   "source": [
    "Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8YB8YFRLG5F",
    "outputId": "50ce2ded-62a7-493a-85b0-8d93f1c9e67a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:loading training data\n",
      "INFO:root:training data loaded\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_TABLE = 'erwinh-ml-demos.ml_sample.training_data_20210726163730'\n",
    "CUSTOM_MODEL_DIR = 'gs://erwin-demo-vertex/model/sample_model3'\n",
    "TENSORBOARD_LOG_DIR = 'gs://erwin-demo-vertex/tensorboard/'\n",
    "model_dir = CUSTOM_MODEL_DIR\n",
    "table_id = TRAINING_DATA_TABLE\n",
    "\n",
    "bqclient = bigquery.Client('erwinh-ml-demos')\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "\n",
    "query_string = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM `{}`\n",
    "\"\"\".format(table_id)\n",
    "logging.info('loading training data')\n",
    "df = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "logging.info('training data loaded');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEr9AtAuLG5F"
   },
   "source": [
    "Train a simple DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSaJXpOULG5F",
    "outputId": "0f0dba3c-a610-45d6-c6bf-f94e1dc47ae0"
   },
   "outputs": [],
   "source": [
    "def getModel(hiddenSize, lr, df, tensorboard_log_dir = None):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hiddenSize, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.AUC(curve='ROC')])\n",
    "    # df['is_churn'] = df['arpu_m4'] < 1\n",
    "    X_train, y_train = df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']][1000:], df['is_churn'][1000:]\n",
    "    callbacks = []\n",
    "    if tensorboard_log_dir != None:\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tensorboard_log_dir,\n",
    "            histogram_freq=1))\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=2, verbose=1, batch_size=10, callbacks=callbacks)\n",
    "    \n",
    "    return model\n",
    "\n",
    "output_directory = model_dir\n",
    "if os.environ.get('AIP_MODEL_DIR') is not None:\n",
    "    output_directory = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "logging.info('Creating and training model ...')\n",
    "model = getModel(hiddenSize=5, lr=0.01, df=df, tensorboard_log_dir=TENSORBOARD_LOG_DIR)\n",
    "tf.saved_model.save(model, 'churn_prediction_model1')\n",
    "\n",
    "model_builder = SavedModelMetadataBuilder(\n",
    "    'churn_prediction_model1')\n",
    "\n",
    "metadata = model_builder.get_metadata()\n",
    "\n",
    "model_builder.save_model_with_metadata(model_dir)\n",
    "\n",
    "result = model.evaluate(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']][:1000], df['is_churn'][:1000])\n",
    "logging.info(f\"result: {result}\")\n",
    "logging.info(model.metrics_names)\n",
    "\n",
    "model_builder.save_model_with_metadata('churn_prediction_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtFZJ3g8LG5G"
   },
   "source": [
    "### 3.1.1 Using managed tensorboard to track experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_fZQtTsLG5G"
   },
   "outputs": [],
   "source": [
    "! pip install -U pip\n",
    "! pip install google-cloud-aiplatform[tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4160QhEXLG5H",
    "outputId": "2f3838ed-0887-4a28-de28-3cab82a7bb1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [9006611840653852672]...done.                            \n",
      "Created Vertex AI Tensorboard: projects/33856278209/locations/us-central1/tensorboards/3198787188456161280.\n"
     ]
    }
   ],
   "source": [
    "! gcloud beta ai tensorboards create --display-name test123 \\\n",
    "  --project erwinh-ml-demos \\\n",
    "  --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAPPSDGELG5H",
    "outputId": "0773c21b-6d3a-46b5-fb0b-5757a638f1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-27 08:04:45.988633: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "View your Tensorboard at https://us-central1.tensorboard.googleusercontent.com/experiment/projects+33856278209+locations+us-central1+tensorboards+3198787188456161280+experiments+test-experiment\n",
      "\u001b[1m[2021-09-27T08:04:47]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2021-09-27T08:04:48]\u001b[0m Total uploaded: 4 scalars, 16 tensors (1.6 kB), 1 binary objects (13.8 kB)\n",
      "\u001b[2K\u001b[33mListening for new data in logdir...\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "! tb-gcp-uploader --tensorboard_resource_name projects/33856278209/locations/us-central1/tensorboards/3198787188456161280 \\\n",
    "  --logdir=gs://gs://erwin-demo-vertex/tensorboard/train \\\n",
    "  --experiment_name=test-experiment --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TbM6k9RLG5H"
   },
   "source": [
    "### 3.1.2 Vizier (auto hyper parameter tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsREaiveLG5I"
   },
   "source": [
    "#### import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbRY_WzQLG5I"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform_v1beta1\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkJYCn2gLG5J"
   },
   "source": [
    "#### Create study configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4teKaODqLG5J",
    "outputId": "caa33853-6934-4d1b-9a9a-1bfeb2cc6f14"
   },
   "outputs": [],
   "source": [
    "# Parameter Configuration\n",
    "STUDY_DISPLAY_NAME = \"{}_study_{}\".format(\n",
    "    PROJECT_ID.replace(\"-\", \"\"), datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")  # @param {type: 'string'}\n",
    "\n",
    "param_lr = {\"parameter_id\": \"lr\", \"double_value_spec\": {\"min_value\": 0.01, \"max_value\": 0.1}}\n",
    "\n",
    "param_hs = {\n",
    "    \"parameter_id\": \"hiddenSize\",\n",
    "    \"integer_value_spec\": {\"min_value\": 3, \"max_value\": 10},\n",
    "}\n",
    "\n",
    "# Objective Metrics\n",
    "metric_auc = {\"metric_id\": \"auc\", \"goal\": \"MAXIMIZE\"}\n",
    "\n",
    "# Put it all together in a study configuration\n",
    "study = {\n",
    "    \"display_name\": STUDY_DISPLAY_NAME,\n",
    "    \"study_spec\": {\n",
    "        \"algorithm\": \"RANDOM_SEARCH\",\n",
    "        \"parameters\": [\n",
    "            param_lr,\n",
    "            param_hs,\n",
    "        ],\n",
    "        \"metrics\": [metric_auc],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(study, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPRAoyINLG5p"
   },
   "source": [
    "#### create study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZoINfAALG5q",
    "outputId": "99652526-c6e8-4d5c-aa7d-a8e15c0e0668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDY_ID: projects/33856278209/locations/us-central1/studies/3996296626522\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT = REGION + \"-aiplatform.googleapis.com\"\n",
    "PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
    "vizier_client = aiplatform_v1beta1.VizierServiceClient(\n",
    "    client_options=dict(api_endpoint=ENDPOINT)\n",
    ")\n",
    "study = vizier_client.create_study(parent=PARENT, study=study)\n",
    "STUDY_ID = study.name\n",
    "print(\"STUDY_ID: {}\".format(STUDY_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2eSmrq4LG5q"
   },
   "source": [
    "#### Set evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbROVHw1LG5r"
   },
   "outputs": [],
   "source": [
    "def MetricAUCEvaluation(hs, lr, df):\n",
    "    model = getModel(hs, lr, df)\n",
    "    result = model.evaluate(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']][:1000], df['is_churn'][:1000])\n",
    "    return result[1]\n",
    "\n",
    "\n",
    "def CreateMetrics(trial_id, hs, lr, df):\n",
    "    print((\"=========== Start Trial: [{}] =============\").format(trial_id))\n",
    "\n",
    "    # Evaluate both objective metrics for this trial\n",
    "    auc = MetricAUCEvaluation(hs, lr, df)\n",
    "    print(\n",
    "        \"[hidden size = {}, lr = {}] => auc = {}\".format(\n",
    "            hs, lr, auc\n",
    "        )\n",
    "    )\n",
    "    metric1 = {\"metric_id\": \"auc\", \"value\": auc}\n",
    "\n",
    "    # Return the results for this trial\n",
    "    return [metric1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U39Ekke1LG5r"
   },
   "source": [
    "#### Set configuration parameters for running trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xa7UBUn4LG5r",
    "outputId": "b7b72a45-7481-4b0c-a3ca-83866de0b3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_id: client1\n",
      "suggestion_count_per_request: 5\n",
      "max_trial_id_to_stop: 4\n"
     ]
    }
   ],
   "source": [
    "client_id = \"client1\"  # @param {type: 'string'}\n",
    "suggestion_count_per_request = 5  # @param {type: 'integer'}\n",
    "max_trial_id_to_stop = 4  # @param {type: 'integer'}\n",
    "\n",
    "print(\"client_id: {}\".format(client_id))\n",
    "print(\"suggestion_count_per_request: {}\".format(suggestion_count_per_request))\n",
    "print(\"max_trial_id_to_stop: {}\".format(max_trial_id_to_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpOnBeC2LG5r",
    "outputId": "f8f43fe3-5ece-44c4-b4ee-97a494f452ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>entity_type_users</th>\n",
       "      <th>mou_m1</th>\n",
       "      <th>mou_m2</th>\n",
       "      <th>mou_m3</th>\n",
       "      <th>arpu_m1</th>\n",
       "      <th>arpu_m2</th>\n",
       "      <th>arpu_m3</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7000002729</td>\n",
       "      <td>8.86</td>\n",
       "      <td>0.06</td>\n",
       "      <td>17.96</td>\n",
       "      <td>172.801</td>\n",
       "      <td>298.935</td>\n",
       "      <td>278.804</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7000008874</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.59</td>\n",
       "      <td>145.56</td>\n",
       "      <td>0.000</td>\n",
       "      <td>163.702</td>\n",
       "      <td>144.836</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7000013153</td>\n",
       "      <td>0.00</td>\n",
       "      <td>84.58</td>\n",
       "      <td>20.98</td>\n",
       "      <td>42.955</td>\n",
       "      <td>303.673</td>\n",
       "      <td>400.007</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7000013547</td>\n",
       "      <td>1666.98</td>\n",
       "      <td>1597.48</td>\n",
       "      <td>2522.23</td>\n",
       "      <td>1308.004</td>\n",
       "      <td>1150.284</td>\n",
       "      <td>1601.142</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7000016253</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.68</td>\n",
       "      <td>26.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103.692</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7002155209</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>247.650</td>\n",
       "      <td>187.647</td>\n",
       "      <td>183.422</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7002246324</td>\n",
       "      <td>9.79</td>\n",
       "      <td>8.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>280.572</td>\n",
       "      <td>100.973</td>\n",
       "      <td>33.990</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7002280506</td>\n",
       "      <td>276.21</td>\n",
       "      <td>56.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>831.464</td>\n",
       "      <td>371.873</td>\n",
       "      <td>0.050</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7002401627</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>540.820</td>\n",
       "      <td>273.201</td>\n",
       "      <td>440.417</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>2021-07-21 14:31:11+00:00</td>\n",
       "      <td>7002409764</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>124.537</td>\n",
       "      <td>77.206</td>\n",
       "      <td>2.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timestamp entity_type_users   mou_m1   mou_m2   mou_m3  \\\n",
       "0     2021-07-21 14:31:11+00:00        7000002729     8.86     0.06    17.96   \n",
       "1     2021-07-21 14:31:11+00:00        7000008874     0.00   108.59   145.56   \n",
       "2     2021-07-21 14:31:11+00:00        7000013153     0.00    84.58    20.98   \n",
       "3     2021-07-21 14:31:11+00:00        7000013547  1666.98  1597.48  2522.23   \n",
       "4     2021-07-21 14:31:11+00:00        7000016253     6.24     0.00    23.68   \n",
       "...                         ...               ...      ...      ...      ...   \n",
       "99994 2021-07-21 14:31:11+00:00        7002155209     0.00     0.00     0.00   \n",
       "99995 2021-07-21 14:31:11+00:00        7002246324     9.79     8.01     0.00   \n",
       "99996 2021-07-21 14:31:11+00:00        7002280506   276.21    56.71     0.00   \n",
       "99997 2021-07-21 14:31:11+00:00        7002401627     2.39     0.25     0.00   \n",
       "99998 2021-07-21 14:31:11+00:00        7002409764     0.53     0.83     0.00   \n",
       "\n",
       "        arpu_m1   arpu_m2   arpu_m3  is_churn  \n",
       "0       172.801   298.935   278.804      True  \n",
       "1         0.000   163.702   144.836      True  \n",
       "2        42.955   303.673   400.007      True  \n",
       "3      1308.004  1150.284  1601.142      True  \n",
       "4        26.150     0.000   103.692      True  \n",
       "...         ...       ...       ...       ...  \n",
       "99994   247.650   187.647   183.422      True  \n",
       "99995   280.572   100.973    33.990     False  \n",
       "99996   831.464   371.873     0.050      True  \n",
       "99997   540.820   273.201   440.417      True  \n",
       "99998   124.537    77.206     2.000      True  \n",
       "\n",
       "[99999 rows x 9 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq5_OY0SLG5w"
   },
   "source": [
    "### Run vertex Vizier trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSxtwldoLG5y",
    "outputId": "b7f646b5-02e3-4076-fed2-797853a6e2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial : hidden size is 10.0, lr is 0.014751820652343813.\n",
      "=========== Start Trial: [projects/33856278209/locations/us-central1/studies/3996296626522/trials/1] =============\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "9900/9900 [==============================] - 26s 3ms/step - loss: 0.7532 - auc_1: 0.7848\n",
      "Epoch 2/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.5303 - auc_1: 0.7793\n",
      "Epoch 3/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.5095 - auc_1: 0.7649\n",
      "Epoch 4/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.5108 - auc_1: 0.7522\n",
      "Epoch 5/10\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 0.4961 - auc_1: 0.7344\n",
      "Epoch 6/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.5127 - auc_1: 0.7259\n",
      "Epoch 7/10\n",
      "9900/9900 [==============================] - 26s 3ms/step - loss: 0.4972 - auc_1: 0.7071\n",
      "Epoch 8/10\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 0.4968 - auc_1: 0.6987\n",
      "Epoch 9/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.4918 - auc_1: 0.6883\n",
      "Epoch 10/10\n",
      "9900/9900 [==============================] - 26s 3ms/step - loss: 0.4793 - auc_1: 0.6708\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4836 - auc_1: 0.6107\n",
      "[hidden size = 10.0, lr = 0.014751820652343813] => auc = 0.6107170581817627\n",
      "Trial : hidden size is 7.0, lr is 0.09845553757419358.\n",
      "=========== Start Trial: [projects/33856278209/locations/us-central1/studies/3996296626522/trials/2] =============\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer dense_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.4874 - auc_2: 0.5078\n",
      "Epoch 2/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3500 - auc_2: 0.4994\n",
      "Epoch 3/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3250 - auc_2: 0.4980\n",
      "Epoch 4/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3248 - auc_2: 0.5040\n",
      "Epoch 5/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3249 - auc_2: 0.4975\n",
      "Epoch 6/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3250 - auc_2: 0.4996\n",
      "Epoch 7/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3248 - auc_2: 0.5010\n",
      "Epoch 8/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3248 - auc_2: 0.5016\n",
      "Epoch 9/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3249 - auc_2: 0.5008\n",
      "Epoch 10/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3249 - auc_2: 0.5019\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2143 - auc_2: 0.5000\n",
      "[hidden size = 7.0, lr = 0.09845553757419358] => auc = 0.5\n",
      "Trial : hidden size is 6.0, lr is 0.04914706983893289.\n",
      "=========== Start Trial: [projects/33856278209/locations/us-central1/studies/3996296626522/trials/3] =============\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3387 - auc_3: 0.5092\n",
      "Epoch 2/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3235 - auc_3: 0.5030\n",
      "Epoch 3/10\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 0.3236 - auc_3: 0.5003\n",
      "Epoch 4/10\n",
      "9900/9900 [==============================] - 26s 3ms/step - loss: 0.3237 - auc_3: 0.4981\n",
      "Epoch 5/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3236 - auc_3: 0.5013\n",
      "Epoch 6/10\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 0.3238 - auc_3: 0.4956\n",
      "Epoch 7/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3236 - auc_3: 0.4997\n",
      "Epoch 8/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3238 - auc_3: 0.5002\n",
      "Epoch 9/10\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 0.3239 - auc_3: 0.4914\n",
      "Epoch 10/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3237 - auc_3: 0.5004\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2226 - auc_3: 0.5000\n",
      "[hidden size = 6.0, lr = 0.04914706983893289] => auc = 0.5\n",
      "Trial : hidden size is 7.0, lr is 0.03208383854666178.\n",
      "=========== Start Trial: [projects/33856278209/locations/us-central1/studies/3996296626522/trials/4] =============\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer dense_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.4650 - auc_4: 0.6319\n",
      "Epoch 2/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3235 - auc_4: 0.5006\n",
      "Epoch 3/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3232 - auc_4: 0.5033\n",
      "Epoch 4/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3236 - auc_4: 0.4984\n",
      "Epoch 5/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3233 - auc_4: 0.4959\n",
      "Epoch 6/10\n",
      "9900/9900 [==============================] - 26s 3ms/step - loss: 0.3232 - auc_4: 0.5045\n",
      "Epoch 7/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3232 - auc_4: 0.4998\n",
      "Epoch 8/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3234 - auc_4: 0.4929\n",
      "Epoch 9/10\n",
      "9900/9900 [==============================] - 26s 3ms/step - loss: 0.3232 - auc_4: 0.5025\n",
      "Epoch 10/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3231 - auc_4: 0.5067\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2169 - auc_4: 0.5000\n",
      "[hidden size = 7.0, lr = 0.03208383854666178] => auc = 0.5\n",
      "Trial : hidden size is 5.0, lr is 0.09215628662023562.\n",
      "=========== Start Trial: [projects/33856278209/locations/us-central1/studies/3996296626522/trials/5] =============\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer dense_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3797 - auc_5: 0.5053\n",
      "Epoch 2/10\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 0.3247 - auc_5: 0.4989\n",
      "Epoch 3/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3245 - auc_5: 0.5037\n",
      "Epoch 4/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3246 - auc_5: 0.5011\n",
      "Epoch 5/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3248 - auc_5: 0.4981\n",
      "Epoch 6/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3248 - auc_5: 0.4985\n",
      "Epoch 7/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3247 - auc_5: 0.4992\n",
      "Epoch 8/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3247 - auc_5: 0.5023\n",
      "Epoch 9/10\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 0.3247 - auc_5: 0.5005\n",
      "Epoch 10/10\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.3245 - auc_5: 0.5003\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2422 - auc_5: 0.5000\n",
      "[hidden size = 5.0, lr = 0.09215628662023562] => auc = 0.5\n"
     ]
    }
   ],
   "source": [
    "trial_id = 0\n",
    "while int(trial_id) < max_trial_id_to_stop:\n",
    "    suggest_response = vizier_client.suggest_trials(\n",
    "        {\n",
    "            \"parent\": STUDY_ID,\n",
    "            \"suggestion_count\": suggestion_count_per_request,\n",
    "            \"client_id\": client_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for suggested_trial in suggest_response.result().trials:\n",
    "        trial_id = suggested_trial.name.split(\"/\")[-1]\n",
    "        trial = vizier_client.get_trial({\"name\": suggested_trial.name})\n",
    "\n",
    "        if trial.state in [\"COMPLETED\", \"INFEASIBLE\"]:\n",
    "            continue\n",
    "        for param in trial.parameters:\n",
    "            if param.parameter_id == \"lr\":\n",
    "                lr = param.value\n",
    "            elif param.parameter_id == \"hiddenSize\":\n",
    "                hs = param.value\n",
    "        print(\"Trial : hidden size is {}, lr is {}.\".format(hs, lr))\n",
    "\n",
    "        vizier_client.add_trial_measurement(\n",
    "            {\n",
    "                \"trial_name\": suggested_trial.name,\n",
    "                \"measurement\": {\n",
    "                    \"metrics\": CreateMetrics(suggested_trial.name, hs, lr, df)\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = vizier_client.complete_trial(\n",
    "            {\"name\": suggested_trial.name, \"trial_infeasible\": False}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIKtrO9nLG5y"
   },
   "source": [
    "---\n",
    "### 3.2 Deploy trained model to Vertex AI endpoint \n",
    "Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LtjGkhxLG5z"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai models upload \\\n",
    "  --region=us-central1  \\\n",
    "  --display-name=churn-model-explain{TIMESTAMP} \\\n",
    "  --container-image-uri=us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest  \\\n",
    "  --artifact-uri={CUSTOM_MODEL_DIR} \\\n",
    "  --explanation-method=sampled-shapley \\\n",
    "  --explanation-path-count=10 \\\n",
    "  --explanation-metadata-file=./churn_prediction_model/explanation_metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UH4mYe-dLG5z"
   },
   "source": [
    "Deploy to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBRQ5-0TLG5z",
    "outputId": "e493683a-e746-4e8b-f532-d9ff943e5c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://{REGION}-aiplatform.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.ai.models.list) The endpoint_overrides property must be an absolute URI beginning with http:// or https:// and ending with a trailing '/'. [https://{REGION}-aiplatform.googleapis.com/] is not a valid endpoint override.\n"
     ]
    }
   ],
   "source": [
    "! gcloud beta ai models list \\\n",
    "  --region={REGION} \\\n",
    "  --filter=display_name=churn-model-explain{TIMESTAMP}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2qBeEhZLG5z",
    "outputId": "6d161866-4782-47a4-a2e0-8de43d65d47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai.models) Invalid choice: 'update'.\n",
      "Maybe you meant:\n",
      "  gcloud ai-platform models update\n",
      "  gcloud ai-platform models add-iam-policy-binding\n",
      "  gcloud ai-platform models create\n",
      "  gcloud ai-platform models delete\n",
      "  gcloud ai-platform models describe\n",
      "  gcloud ai-platform models get-iam-policy\n",
      "  gcloud ai-platform models list\n",
      "  gcloud ai-platform models remove-iam-policy-binding\n",
      "  gcloud ai-platform models set-iam-policy\n",
      "\n",
      "To search the help text of gcloud commands, run:\n",
      "  gcloud help -- SEARCH_TERMS\n"
     ]
    }
   ],
   "source": [
    "! gcloud  ai models  update 5699789113553584128 \\\n",
    "--region=us-central1 \\\n",
    "--update-labels backend=webserver,media=images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONRFH-qrLG50",
    "outputId": "28ee34de-5c9f-4753-e985-0f5850860bfb"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = '7595188830165008384'\n",
    "model = aiplatform.Model(MODEL_ID)\n",
    "model.labels = {'aba':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YRSVANxLG50"
   },
   "outputs": [],
   "source": [
    "# Copy and paste the model ID from the output of last block in the MODEL_ID field\n",
    "MODEL_ID = '7595188830165008384'\n",
    "model = aiplatform.Model(MODEL_ID)\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    traffic_percentage=100,\n",
    "    deployed_model_display_name='churn-model-explain'+TIMESTAMP,\n",
    "    machine_type=\"n1-standard-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgd_w0JdLG50"
   },
   "source": [
    "---\n",
    "### 3.3 Make explainable prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rqr7PQhLG50"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai endpoints list \\\n",
    "  --region={REGION} \\\n",
    "  --filter=display_name=churn-model-explain{TIMESTAMP}_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtcHZmN0LG51"
   },
   "outputs": [],
   "source": [
    "# Copy and paste the endpoint ID from the output of last block in the ENDPOINT_ID field\n",
    "ENDPOINT_ID = '5680921494020947968'\n",
    "vertex_model = explainable_ai_sdk.load_model_from_vertex(\n",
    "    project=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    endpoint_id=ENDPOINT_ID\n",
    ")\n",
    "\n",
    "\n",
    "response = vertex_model.explain(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']].values.tolist()[:10])\n",
    "response[0].visualize_attributions()\n",
    "response[0].feature_importance()\n",
    "print(response[0].as_tensors())\n",
    "print(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']].values.tolist()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OQi6Gr0LG51"
   },
   "source": [
    "---\n",
    "### 3.4 Explain the model: What-if tool\n",
    "#### 3.4.1 Install the What-if tool and jupyter widget\n",
    "<font color='red'>Note that if it's the first time you use What-if tool in notebook, you may need to restart the browser tab</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbjzRXzULG51"
   },
   "outputs": [],
   "source": [
    "!pip install witwidget witwidget-gpu\n",
    "!sudo /opt/conda/bin/jupyter labextension install wit-widget\n",
    "!sudo /opt/conda/bin/jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfPcc1wFLG52"
   },
   "outputs": [],
   "source": [
    "df.head()\n",
    "endpoint.predict(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']].values.tolist()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5Q98twaLG52"
   },
   "source": [
    "#### 3.4.2 Run What-if Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFYTtlRbLG52"
   },
   "outputs": [],
   "source": [
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "def predict_func(feature):\n",
    "    result = endpoint.predict(feature)\n",
    "    return result[0]\n",
    "    \n",
    "config_builder = (WitConfigBuilder(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3', 'is_churn']].values.tolist()[-100:], ['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3', 'is_churn'])\n",
    "  .set_custom_predict_fn(predict_func)\n",
    "  .set_target_feature('is_churn')\n",
    "  .set_label_vocab(['churn', 'not_churn']))\n",
    "WitWidget(config_builder, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpaD14TxZyVm"
   },
   "source": [
    "---\n",
    "---\n",
    "## 4. Vertex Pipeline\n",
    "Create a vertex pipeline that orchestrate feature serving, model training, evaluation, and deployment\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Create components with Kubeflow\n",
    "#### 4.1.1 Feature serving component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUV-ST_-LG53"
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_SELECTOR_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m_ZU0GzBMRi"
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_SELECTOR_LOC = BUCKET_NAME + '/dataset/query_instance_2.csv'\n",
    "\n",
    "@component(base_image='gcr.io/deeplearning-platform-release/tf2-gpu.2-5',\n",
    "           packages_to_install=[\n",
    "               'pandas', \n",
    "               \"google-cloud-bigquery-storage==2.4.0\",\n",
    "               'google-cloud-bigquery', \n",
    "               'pyarrow ',\n",
    "               'fsspec', \n",
    "               'gcsfs', \n",
    "               'google-cloud-aiplatform==1.1.0'])\n",
    "def feature_store_serve_op(api_endpoint: str, \n",
    "    project_id: str, \n",
    "    region: str, \n",
    "    featurestore_id: str, \n",
    "    bq_feature_source: str,\n",
    "    bq_training_output: str,\n",
    "    training_data_selector_loc: str = 'gs://erwin-demo-vertex/data/query_instance_2.csv'):\n",
    "    import google.auth\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import bigquery_storage\n",
    "    from google.cloud.aiplatform_v1beta1 import (\n",
    "        FeaturestoreOnlineServingServiceClient, FeaturestoreServiceClient)\n",
    "    from google.cloud.aiplatform_v1beta1.types import FeatureSelector, IdMatcher\n",
    "    from google.cloud.aiplatform_v1beta1.types import \\\n",
    "        entity_type as entity_type_pb2\n",
    "    from google.cloud.aiplatform_v1beta1.types import feature as feature_pb2\n",
    "    from google.cloud.aiplatform_v1beta1.types import \\\n",
    "        featurestore as featurestore_pb2\n",
    "    from google.cloud.aiplatform_v1beta1.types import \\\n",
    "        featurestore_monitoring as featurestore_monitoring_pb2\n",
    "    from google.cloud.aiplatform_v1beta1.types import \\\n",
    "        featurestore_online_service as featurestore_online_service_pb2\n",
    "    from google.cloud.aiplatform_v1beta1.types import \\\n",
    "        featurestore_service as featurestore_service_pb2\n",
    "    from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"feature store serve task\")\n",
    "\n",
    "    bqclient = bigquery.Client(project_id)\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    query_string = \"\"\"\n",
    "    SELECT\n",
    "        mobile_number\n",
    "    FROM `{}`\n",
    "    \"\"\".format(bq_feature_source.split('/')[-1])\n",
    "\n",
    "    user_df = (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "\n",
    "    X_train = user_df['mobile_number']\n",
    "    \n",
    "    now = datetime.now()\n",
    "\n",
    "    current_time = now.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    res = pd.DataFrame()\n",
    "    res['users']  = X_train\n",
    "    res['timestamp'] = current_time\n",
    "    res.to_csv(training_data_selector_loc, index=False)\n",
    "    admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": api_endpoint})\n",
    "    batch_serving_request = featurestore_service_pb2.BatchReadFeatureValuesRequest(\n",
    "        # featurestore info\n",
    "        featurestore=admin_client.featurestore_path(project_id, region, featurestore_id),\n",
    "        # URL for the label data, i.e., Table 1.\n",
    "        csv_read_instances=io_pb2.CsvSource(\n",
    "            gcs_source=io_pb2.GcsSource(uris=['gs://erwin-demo-vertex/data/query_instance_2.csv'])\n",
    "        ), \n",
    "        destination=featurestore_service_pb2.FeatureValueDestination(\n",
    "            bigquery_destination=io_pb2.BigQueryDestination(\n",
    "                # Output to BigQuery table created earlier\n",
    "                output_uri=bq_training_output\n",
    "            )\n",
    "        ),\n",
    "        entity_type_specs=[\n",
    "            featurestore_service_pb2.BatchReadFeatureValuesRequest.EntityTypeSpec(\n",
    "                entity_type_id=\"users\",\n",
    "                feature_selector=FeatureSelector(\n",
    "                    id_matcher=IdMatcher(\n",
    "                        ids=[\n",
    "                            # features, use \"*\" if you want to select all features within this entity type\n",
    "                            \"mou_m1\",\n",
    "                            \"mou_m2\",\n",
    "                            \"mou_m3\",\n",
    "                            \"arpu_m1\",\n",
    "                            \"arpu_m2\",\n",
    "                            \"arpu_m3\",\n",
    "                            \"is_churn\"\n",
    "                        ]\n",
    "                    )\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    batch_serving_lro = admin_client.batch_read_feature_values(batch_serving_request)\n",
    "    batch_serving_lro.result()\n",
    "\n",
    "@component(base_image='gcr.io/deeplearning-platform-release/tf2-gpu.2-5',\n",
    "           packages_to_install=['google-cloud-aiplatform==1.1.0'])\n",
    "def evaluate_model(\n",
    "    api_endpoint: str, \n",
    "    project_id: str, \n",
    "    region: str, \n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics]):\n",
    "    model_id = model.uri.split('/')[-1]\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    from google.cloud import aiplatform\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    name = client.model_path(\n",
    "        project=project_id, location=region, model=model_id\n",
    "    )\n",
    "    print(name)\n",
    "    response = client.list_model_evaluations(parent=name)\n",
    "    print(\"response:\", response)\n",
    "    print(model)\n",
    "    print(model.metadata)\n",
    "    print(model.uri)\n",
    "    print(model.name)\n",
    "    auRoc = response.__getattr__('model_evaluations')[0].metrics['auRoc']\n",
    "    metrics.log_metric(\"auROC\", float(auRoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qG_7WH1XLG53"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbDwe23VLG53"
   },
   "outputs": [],
   "source": [
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "from google.cloud.aiplatform_v1beta1 import (\n",
    "    FeaturestoreOnlineServingServiceClient, FeaturestoreServiceClient)\n",
    "from google.cloud.aiplatform_v1beta1.types import FeatureSelector, IdMatcher\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    entity_type as entity_type_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore as featurestore_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_monitoring as featurestore_monitoring_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_online_service as featurestore_online_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_service as featurestore_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "bq_feature_source = BQ_FEATURE_SOURCE\n",
    "bq_training_output = BQ_TRAINING_OUTPUT\n",
    "bq_feature_source_table = BQ_FEATURE_TABLE\n",
    "project = PROJECT_ID\n",
    "model_display_name = MODEL_DISPLAY_NAME\n",
    "api_endpoint = API_ENDPOINT\n",
    "region = REGION\n",
    "serving_container_image_uri = \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest\"\n",
    "featurestore_id = FEATURE_STORE\n",
    "training_data_selector_loc = 'gs://erwin-demo-vertex/data/query_instance_2.csv'\n",
    "api_endpoint=api_endpoint\n",
    "project_id=project\n",
    "region=region\n",
    "featurestore_id=featurestore_id\n",
    "bq_feature_source=bq_feature_source\n",
    "bq_training_output=bq_training_output\n",
    "        \n",
    "print(\"feature store serve task\")\n",
    "\n",
    "bqclient = bigquery.Client(project_id)\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "query_string = \"\"\"\n",
    "SELECT\n",
    "    mobile_number\n",
    "FROM `{}`\n",
    "\"\"\".format(bq_feature_source.split('/')[-1])\n",
    "\n",
    "user_df = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "\n",
    "X_train = user_df['mobile_number']\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(\"Current Time =\", current_time)\n",
    "res = pd.DataFrame()\n",
    "res['users']  = X_train\n",
    "res['timestamp'] = current_time\n",
    "res.to_csv(training_data_selector_loc, index=False)\n",
    "admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": api_endpoint})\n",
    "batch_serving_request = featurestore_service_pb2.BatchReadFeatureValuesRequest(\n",
    "    # featurestore info\n",
    "    featurestore=admin_client.featurestore_path(project_id, region, featurestore_id),\n",
    "    # URL for the label data, i.e., Table 1.\n",
    "    csv_read_instances=io_pb2.CsvSource(\n",
    "        gcs_source=io_pb2.GcsSource(uris=['gs://erwin-demo-vertex/data/query_instance_2.csv'])\n",
    "    ),\n",
    "    destination=featurestore_service_pb2.FeatureValueDestination(\n",
    "        bigquery_destination=io_pb2.BigQueryDestination(\n",
    "            # Output to BigQuery table created earlier\n",
    "            output_uri=bq_training_output\n",
    "        )\n",
    "    ),\n",
    "    entity_type_specs=[\n",
    "        featurestore_service_pb2.BatchReadFeatureValuesRequest.EntityTypeSpec(\n",
    "            entity_type_id=\"users\",\n",
    "            feature_selector=FeatureSelector(\n",
    "                id_matcher=IdMatcher(\n",
    "                    ids=[\n",
    "                        # features, use \"*\" if you want to select all features within this entity type\n",
    "                        \"mou_m1\",\n",
    "                        \"mou_m2\",\n",
    "                        \"mou_m3\",\n",
    "                        \"arpu_m1\",\n",
    "                        \"arpu_m2\",\n",
    "                        \"arpu_m3\",\n",
    "                        \"is_churn\"\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "batch_serving_lro = admin_client.batch_read_feature_values(batch_serving_request)\n",
    "batch_serving_lro.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j60xYiBLLG54"
   },
   "source": [
    "#### 4.1.2 Customized Training Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uv8aDNh7LG54"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import datasets\n",
    "from google.cloud import aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import ClassificationMetrics, Metrics, Output, Input, component, Model, Dataset, Artifact, Condition\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "CUSTOM_MODEL_DIR = 'gs://erwin-demo-vertex/model'+TIMESTAMP\n",
    "TENSORBOARD_LOG_DIR = 'gs://erwin-demo-vertex/tensorboard/'\n",
    "\n",
    "@component(base_image='gcr.io/deeplearning-platform-release/tf2-gpu.2-5',\n",
    "           packages_to_install=[\n",
    "                                'pandas', \n",
    "                                \"google-cloud-bigquery-storage==2.4.0\",\n",
    "                                'google-cloud-bigquery', \n",
    "                                'pyarrow',\n",
    "                                'fsspec', \n",
    "                                'gcsfs', \n",
    "                                'google-cloud-aiplatform==1.1.0',\n",
    "                                'kfp==1.6.3',\n",
    "                                'kfp-pipeline-spec',\n",
    "                                'kfp-server-api',\n",
    "                                'pandas==1.2.4',\n",
    "                                'pandas-profiling==3.0.0',\n",
    "                                'tensorboard==2.5.0',\n",
    "                                'tensorboard-data-server==0.6.1',\n",
    "                                'tensorboard-plugin-wit==1.8.0',\n",
    "                                'tensorflow==2.5.0',\n",
    "                                'tensorflow-datasets==4.3.0',\n",
    "                                'tensorflow-estimator==2.5.0',\n",
    "                                'tensorflow-metadata==1.0.0',\n",
    "                                'google-cloud-logging==2.4.0',\n",
    "                                'explainable-ai-sdk==1.3.0',\n",
    "                                'numpy==1.19.5',\n",
    "                                'sklearn'])\n",
    "def custom_training_op(\n",
    "    project_id: str,\n",
    "    dataset: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model_dir: str,\n",
    "    tensorboard_log_dir: str,\n",
    "):\n",
    "    from google.cloud.aiplatform import datasets\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import bigquery_storage\n",
    "    from kfp.v2.dsl import ClassificationMetrics, Metrics, Output, component\n",
    "    import numpy as np\n",
    "    \n",
    "    dataset = aiplatform.TabularDataset('projects/' + dataset.uri.split('projects/')[-1])\n",
    "    table_id = dataset._gca_resource.metadata.get(\"inputConfig\").get(\"bigquerySource\").get(\"uri\").split('//')[-1]\n",
    "    \n",
    "    bqclient = bigquery.Client(project_id)\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "\n",
    "    query_string = \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM `{}`\n",
    "    \"\"\".format(table_id)\n",
    "\n",
    "    df = (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "\n",
    "    output_directory = model_dir\n",
    "    if os.environ.get('AIP_MODEL_DIR') is not None:\n",
    "        output_directory = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "    logging.info('Creating and training model ...')\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(5, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    log_dir = tensorboard_log_dir\n",
    "    if os.environ.get('AIP_TENSORBOARD_LOG_DIR') is not None:\n",
    "        log_dir = os.environ[\"AIP_TENSORBOARD_LOG_DIR\"]\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir,\n",
    "      histogram_freq=1)\n",
    "\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.AUC()])\n",
    "    X_train, y_train = df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']][1000:], df['is_churn'][1000:]\n",
    "    model.fit(X_train, y_train, epochs=10, verbose=1, batch_size=10, callbacks=[tensorboard_callback])\n",
    "\n",
    "    logging.info(f'Exporting SavedModel to: {output_directory}')\n",
    "    model.save(output_directory)\n",
    "\n",
    "    result = model.evaluate(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']][:1000], df['is_churn'][:1000])\n",
    "    \n",
    "    metrics.log_metric(\"auROC\", result[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aylxgnF8LG54"
   },
   "source": [
    "---\n",
    "### 4.2 Build Pipeline With KFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYkPcRWZLG55"
   },
   "outputs": [],
   "source": [
    "FEATURE_STORE = \"telecom_churn_prediction_20210708091932\"\n",
    "BQ_FEATURE_SOURCE = \"bq://erwinh-ml-demos.telecom_churn.processed_features_9\"\n",
    "BQ_TRAINING_OUTPUT = \"bq://erwinh-ml-demos.telecom_churn.training_data_\"+TIMESTAMP\n",
    "BQ_FEATURE_TABLE = BQ_FEATURE_SOURCE.split('/')[-1]\n",
    "MODEL_DISPLAY_NAME = 'sample-churn-prediction-model'\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"train\" + str(uuid.uuid4()))\n",
    "def pipeline(featurestore_id: str = FEATURE_STORE, \n",
    "    bq_feature_source: str = BQ_FEATURE_SOURCE,\n",
    "    bq_training_output: str = BQ_TRAINING_OUTPUT,\n",
    "    bq_feature_source_table: str = BQ_FEATURE_TABLE,\n",
    "    project: str = PROJECT_ID,\n",
    "    model_display_name: str = MODEL_DISPLAY_NAME,\n",
    "    api_endpoint: str = API_ENDPOINT, \n",
    "    region: str = REGION,\n",
    "    serving_container_image_uri: str = \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest\",\n",
    "):\n",
    "    feature_store_serve_job = feature_store_serve_op(\n",
    "        api_endpoint=api_endpoint, \n",
    "        project_id=project, \n",
    "        region=region, \n",
    "        featurestore_id=featurestore_id, \n",
    "        bq_feature_source=bq_feature_source,\n",
    "        bq_training_output=bq_training_output)\n",
    "    \n",
    "    dataset_create_job = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name='dataset_creation_op',\n",
    "        bq_source=bq_training_output\n",
    "    )\n",
    "    \n",
    "#     dataset_create_job.after(feature_store_serve_job)\n",
    "\n",
    "    custom_training_job = custom_training_op(\n",
    "        project_id=project,\n",
    "        dataset=dataset_create_job.outputs[\"dataset\"],\n",
    "        model_dir=WORKING_DIR,\n",
    "        tensorboard_log_dir=TENSORBOARD_LOG_DIR,\n",
    "    )\n",
    "    \n",
    "    custom_training_job.after(dataset_create_job)\n",
    "    \n",
    "    endpoint_create_job = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "    \n",
    "    automl_training_job = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name='automl_training_op',\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1,\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"mou_m1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"mou_m2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"mou_m3\"}},\n",
    "            {\"numeric\": {\"column_name\": \"arpu_m1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"arpu_m2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"arpu_m3\"}},\n",
    "            {\"categorical\": {\"column_name\": \"is_churn\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_job.outputs[\"dataset\"],\n",
    "        target_column=\"is_churn\",\n",
    "    )\n",
    "    automl_training_job.after(dataset_create_job)\n",
    "\n",
    "    evaluate_model(\n",
    "        api_endpoint=api_endpoint, \n",
    "        project_id=project, \n",
    "        region=region, \n",
    "        model=automl_training_job.outputs[\"model\"])\n",
    "    \n",
    "    automl_model_deploy_job = gcc_aip.ModelDeployOp(\n",
    "        traffic_percentage=100,\n",
    "        endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "        model=automl_training_job.outputs[\"model\"],\n",
    "        project=project,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "    custom_model_upload_job = gcc_aip.ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=WORKING_DIR,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_environment_variables={\"NOT_USED\": \"NO_VALUE\"},\n",
    "    )\n",
    "    custom_model_upload_job.after(custom_training_job)\n",
    "    custom_model_deploy_job = gcc_aip.ModelDeployOp(\n",
    "        project=project,\n",
    "        traffic_percentage=50,\n",
    "        endpoint=automl_model_deploy_job.outputs[\"endpoint\"],\n",
    "        model=custom_model_upload_job.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_display_name,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hl1iYEKSzjP"
   },
   "source": [
    "---\n",
    "### 4.3 Compile and upload pipeline to Vertex AI\n",
    "\n",
    "Now, you're ready to compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycRc83B6bbfO"
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"telecom-sample-pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfNuzFswBB4g"
   },
   "source": [
    "The pipeline compilation generates the `train_upload_deploy.json` job spec file.\n",
    "\n",
    "Next, instantiate an API client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hl5Q74_gkW2c"
   },
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2ilgkB8LG58"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jrn6saiQsPh"
   },
   "source": [
    "Then, you run the defined pipeline like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4Ha4FoDQpkd"
   },
   "outputs": [],
   "source": [
    "\n",
    "response = api_client.create_run_from_job_spec( \"telecom-sample-pipeline.json\", pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        'bq_feature_source': BQ_FEATURE_SOURCE,\n",
    "        \"bq_training_output\": BQ_TRAINING_OUTPUT,\n",
    "        'featurestore_id': FEATURE_STORE,\n",
    "    },\n",
    "    enable_caching=False)\n",
    "#response = api_client.create_schedule_from_job_spec(job_spec_path = \"train_upload_deploy.json\",\n",
    "#                                  schedule=\"2 * * * *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4jxmfyT26gj"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "- Delete Cloud Storage objects that were created.  Uncomment and run the command in the cell below **only if you are not using the `PIPELINE_ROOT` path for any other purpose**.\n",
    "- Delete your deployed model: first, undeploy it from its *endpoint*, then delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtZCXIi1aULZ"
   },
   "outputs": [],
   "source": [
    "# Warning: this command will delete ALL Cloud Storage objects under the PIPELINE_ROOT path.\n",
    "# ! gsutil -m rm -r $PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EHdeZUeLG59"
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model(model_name='9070328814361378816')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlIecE3pLG59"
   },
   "outputs": [],
   "source": [
    "endpoints = aiplatform.Endpoint.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2DJ6YtwLG59"
   },
   "outputs": [],
   "source": [
    "for endpoint in endpoints:\n",
    "    try:\n",
    "        endpoint.delete(force=True)\n",
    "    except:\n",
    "        print(\"delete fail~!!!! \")\n",
    "        print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzB9lbU-LG59"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[Sharing] Vertex-ai-e2e-demo (1) (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:latest"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

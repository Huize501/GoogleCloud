{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72095256-e09b-4295-b2a1-dd40c1baa68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4c61c-ea53-422e-8e01-95c8490ca4f5",
   "metadata": {},
   "source": [
    "# 02 Formalization: Build an end-to-end Churn Pipeline using Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4110d-afcc-4340-a628-8e06dbf22953",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook is a follow-up of `01_feature_management`. In this Notebook we will show you how you can do model experimentation using Vertex AI. \n",
    "\n",
    "**IMPORTANT** You can only run this notebook if you first completed `01_feature_management`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fd8c1-e693-4e31-8ef2-0ab1ea0a5583",
   "metadata": {},
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f0b75-a8c0-4318-9268-a4488fc09828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp --upgrade -q\n",
    "! pip3 install {USER_FLAG} google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0d9c6-2836-4934-9cfb-40fc440f2c1e",
   "metadata": {},
   "source": [
    "### Check installed package versions\n",
    "Run the following cell to check the KFP version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e54a0-cb7f-40e7-bf03-3ef3de18efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze | grep kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193bc5c-c443-4bdf-91b7-780c845d0541",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69e06a-2d48-4a2b-ac9a-1ee3c8aae72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs. Might take a bit. \n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b23acdc-398b-4808-9c27-86cacd8d2c49",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9da0e0-9ec7-4f41-84a7-bba414b80100",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d1166-424c-4256-8b54-9c805453e589",
   "metadata": {},
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6846bf-6028-460e-9990-8a06f4b4c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86396236-b5fb-499d-bb65-83282af1be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f677eb-b7f9-4548-b967-03b7e60e155d",
   "metadata": {},
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5016a-328b-4f0f-9451-9a201108a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"} --> You can change the region if you want\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\"\n",
    "    \n",
    "print(f\"Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5e81c-ddd9-476e-8f6b-dec0526f1281",
   "metadata": {},
   "source": [
    "#### UUID\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec33dc4-1482-4a9a-9a18-30b88f53ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690a0b8-8f90-4308-a4a2-c5cc4dc20843",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. \n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01528833-8b7d-4d19-93cb-867cf235e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f019186-33d8-4ea1-ae6c-751001fdd410",
   "metadata": {},
   "source": [
    "### Load \n",
    "Next we will load the constants and the config file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbb78c-daee-48b0-861c-894314fe33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=f\"{PROJECT_ID}-vertexai-churn\"\n",
    "\n",
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "!gsutil cp ./telecom_churn_data.csv gs://{BUCKET_NAME}/data/\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e7901-ec40-4d37-b48e-37c2fe904311",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "STAGING_URI = f\"gs://{BUCKET_NAME}/staging\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/{UUID}\"\n",
    "MODEL_DIR = f\"gs://{BUCKET_NAME}/model/{UUID}\"\n",
    "MODEL_DISPLAY_NAME = f\"train_deploy_{UUID}\"\n",
    "TRAINING_DATA_TABLE = f\"bq://{PROJECT_ID}.ml_sample.train\" \n",
    "TABLE_ID = f\"{PROJECT_ID}.ml_sample.train\"\n",
    "TENSORBOARD_LOG_DIR = f\"{BUCKET_NAME}/tensorboard_log_dir/\"\n",
    "DISPLAY_NAME = \"telecom_churn_pipeline\"\n",
    "TRAINING_DATA_SELECTOR_LOC = f\"gs://{BUCKET_NAME}/dataset/query_instance_2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6adf7-9dba-4d19-8d86-d62af74a1b15",
   "metadata": {},
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a662630-9629-4085-a97f-4d0f7af8d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feec9fb-be73-44a0-a39d-3f909618183f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cb766-5629-46db-a62c-cc9b4be43e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "#import tensorflow as tf\n",
    "import pandas as pd\n",
    "from typing import NamedTuple\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import aiplatform as vertex\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "from google.cloud.aiplatform import datasets\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp, ModelDeployOp)\n",
    "from google.cloud.aiplatform_v1beta1 import (FeaturestoreOnlineServingServiceClient, FeaturestoreServiceClient)\n",
    "from google.cloud.aiplatform import Feature, Featurestore\n",
    "\n",
    "# KFP\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import HTML, Artifact, Condition, Input, Output, component, Metrics, Dataset\n",
    "from kfp.v2.components import importer_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6538d27-6bb9-4932-8930-c2242f2bce09",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe014f-006b-4d27-b70c-820eafb2b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0112a4e-69ac-4aa1-a652-7e602387c76e",
   "metadata": {},
   "source": [
    "### Extract data from the Feature Store\n",
    "The next step is for us to create a dataset in BigQuery that we can use for training. To read data from the feature store we need a `read-instance list` that contains information for each training example. It lists observations at a particular point in time. This can be either a CSV file or a BigQuery table. The list must include the following information:\n",
    "* Timestamps: the times at which labels were observed or measured. The timestamps are required so that Vertex AI Feature Store can perform a point-in-time lookup.\n",
    "* Entity IDs: one or more IDs of the entities that correspond to the label.\n",
    "\n",
    "**Important** Please make sure that you first finish Notebook `01_feature_management`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0377f-e26b-43d4-b6e4-d5db250a0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "query_string = \"\"\"\n",
    "SELECT\n",
    "    mobile_number\n",
    "FROM `{}`\n",
    "\"\"\".format(BQ_RAW_DATA.split('/')[-1])\n",
    "\n",
    "user_df = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "\n",
    "X_train = user_df[\"mobile_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9a8fb-13a0-4c66-9b32-2324403abb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "res = pd.DataFrame()\n",
    "res['users']  = X_train\n",
    "res['timestamp'] = current_time\n",
    "res.to_csv(TRAINING_DATA_SELECTOR_LOC, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bb9ff-f99d-4541-97b0-136309b545db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = Featurestore(\n",
    "    featurestore_name=FEATURESTORE_ID,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "print(fs.gca_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93e46e-70cf-477f-999a-e33cd5968cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FEATURE_IDS = {\n",
    "    # to choose all the features use 'entity_type_id: ['*']'\n",
    "    \"users\": [\"mou_m1\", \"mou_m2\", \"mou_m3\", \"arpu_m1\", \"arpu_m2\", \"arpu_m3\", \"is_churn\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad618cc2-1336-492e-9d66-8d3567064e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.batch_serve_to_bq(\n",
    "    bq_destination_output_uri=TRAINING_DATA_TABLE,\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_uri=TRAINING_DATA_SELECTOR_LOC,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84c3c5-ef1c-4b91-b50e-0cdda4d9f6de",
   "metadata": {},
   "source": [
    "## Build a customer component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689dc44-3f29-495b-894b-0c9d122590ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='gcr.io/deeplearning-platform-release/tf2-gpu.2-5',\n",
    "           packages_to_install=[\n",
    "                                'pandas', \n",
    "                                \"google-cloud-bigquery-storage\",\n",
    "                                'google-cloud-bigquery', \n",
    "                                'pyarrow',\n",
    "                                'fsspec', \n",
    "                                'gcsfs', \n",
    "                                'google-cloud-aiplatform',\n",
    "                                'kfp==1.8.13',\n",
    "                                'kfp-pipeline-spec',\n",
    "                                'kfp-server-api',\n",
    "                                'pandas==1.2.4',\n",
    "                                'pandas-profiling==3.0.0',\n",
    "                                'tensorboard==2.5.0',\n",
    "                                'tensorboard-data-server==0.6.1',\n",
    "                                'tensorboard-plugin-wit==1.8.0',\n",
    "                                'tensorflow==2.5.0',\n",
    "                                'tensorflow-datasets==4.3.0',\n",
    "                                'tensorflow-estimator==2.5.0',\n",
    "                                'tensorflow-metadata==1.0.0',\n",
    "                                'google-cloud-logging==2.4.0',\n",
    "                                'numpy==1.19.5',\n",
    "                                'sklearn'])\n",
    "\n",
    "def custom_training(project_id: str,\n",
    "                    table_id: str,\n",
    "                    model_dir: str,\n",
    "                    metrics: Output[Metrics],\n",
    "                    tensorboard_log_dir: str,) -> NamedTuple(\"Ouptputs\", [(\"output_uri\", str),],): \n",
    "    from google.cloud import aiplatform as vertex\n",
    "    import logging\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import bigquery_storage\n",
    "    from kfp.v2.dsl import ClassificationMetrics, Metrics, Output, component\n",
    "    import numpy as np\n",
    "    \n",
    "    bqclient = bigquery.Client(project_id)\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "\n",
    "    query_string = f\"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM `{table_id}`\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "\n",
    "    if os.environ.get('AIP_MODEL_DIR') is not None:\n",
    "        model_dir = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "    logging.info('Creating and training model ...')\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(.2),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(.2),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    log_dir = tensorboard_log_dir\n",
    "    if os.environ.get('AIP_TENSORBOARD_LOG_DIR') is not None:\n",
    "        log_dir = os.environ[\"AIP_TENSORBOARD_LOG_DIR\"]\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir,\n",
    "      histogram_freq=1)\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    print(df.head(3))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.AUC()])\n",
    "    X_train, y_train = df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']][1000:], df['is_churn'][1000:]\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=2, verbose=1, batch_size=8, callbacks=[tensorboard_callback])\n",
    "    logging.info(f'Exporting SavedModel to: {model_dir}')\n",
    "    model.save(model_dir)\n",
    "\n",
    "    result = model.evaluate(df[['mou_m1','mou_m2', 'mou_m3', 'arpu_m1', 'arpu_m2', 'arpu_m3']][:1000], df['is_churn'][:1000])\n",
    "    \n",
    "    component_outputs = NamedTuple(\"Ouptputs\",\n",
    "                                [(\"output_uri\", str),],)\n",
    "    \n",
    "    metrics.log_metric(\"auROC\", result[0])\n",
    "    \n",
    "    return component_outputs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118c689-c003-48e6-afb3-d8c4dea3b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"telecom-churn-model-pipeline\")\n",
    "def pipeline(project: str = PROJECT_ID,\n",
    "             bq_training_data: str = TABLE_ID,\n",
    "             bq_training_dat_uri: str = TRAINING_DATA_TABLE,\n",
    "             model_display_name: str = MODEL_DISPLAY_NAME,\n",
    "             region: str = REGION,\n",
    "):             \n",
    "        dataset_create_op = vertex_ai_components.TabularDatasetCreateOp(\n",
    "            project=project, display_name='churn_datasets',\n",
    "            location=region, bq_source=bq_training_dat_uri)\n",
    "                \n",
    "        custom_training_op = custom_training(\n",
    "            project_id=project,\n",
    "            table_id=bq_training_data,\n",
    "            model_dir=MODEL_DIR,\n",
    "            tensorboard_log_dir=TENSORBOARD_LOG_DIR,\n",
    "        )\n",
    "        \n",
    "        custom_training_op.after(dataset_create_op)\n",
    "        \n",
    "        unmanaged_model_importer = importer_node.importer(\n",
    "            artifact_uri=custom_training_op.outputs[\"output_uri\"],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest\"\n",
    "                }\n",
    "            },\n",
    "        )    \n",
    "        \n",
    "        unmanaged_model_importer.after(custom_training_op)\n",
    "\n",
    "        model_upload_op = ModelUploadOp(\n",
    "            project=project,\n",
    "            display_name=model_display_name,\n",
    "            unmanaged_container_model=unmanaged_model_importer.outputs[\"artifact\"],\n",
    "        )   \n",
    "        \n",
    "        model_upload_op.after(unmanaged_model_importer)\n",
    "        \n",
    "        endpoint_create_op = EndpointCreateOp(\n",
    "            project=project,\n",
    "            display_name=\"pipelines-created-endpoint\",\n",
    "        )\n",
    "                \n",
    "        custom_model_deploy_job =  ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            traffic_split={\"0\": 100},\n",
    "            deployed_model_display_name=model_display_name,\n",
    "            dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "        )\n",
    "        \n",
    "        custom_model_deploy_job.after(model_upload_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918d215-155f-41f4-95dd-6ca709914591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"telecom_churn_model_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2261be-a9c6-40be-b561-47a272f99d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"telecom_churn_model_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True\n",
    ")\n",
    "\n",
    "print (job.run())\n",
    "\n",
    "! rm telecom_churn_model_pipeline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "COUNTER = 25 # You can change this \n",
    "\n",
    "while COUNTER >= 1:\n",
    "    print (COUNTER)\n",
    "    COUNTER = COUNTER - 1\n",
    "    \n",
    "    mou_m1 = round(random.uniform(-200, 200000), 2)\n",
    "    mou_m2 = round(random.uniform(-200, 200000), 2)\n",
    "    mou_m3 = round(random.uniform(-20000, 200000), 2)\n",
    "    arpu_m1 = round(random.uniform(-1500, 5000), 2)\n",
    "    arpu_m2 = round(random.uniform(-15000, 50000), 2) \n",
    "    arpu_m3 = round(random.uniform(-1500, 5000), 2)\n",
    "    \n",
    "    instances = [[mou_m1,mou_m2,mou_m3,arpu_m1,arpu_m2,arpu_m3],\n",
    "                 [mou_m1,mou_m2,mou_m3,arpu_m1,arpu_m2,arpu_m3],\n",
    "                 [mou_m1,mou_m2,mou_m3,arpu_m1,arpu_m2,arpu_m3],\n",
    "                 [mou_m1,mou_m2,mou_m3,arpu_m1,arpu_m2,arpu_m3]]\n",
    "    \n",
    "    endpoint=vertex.Endpoint.list(order_by=\"update_time\")\n",
    "    endpoint=endpoint[-1]\n",
    "\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    \n",
    "    print(COUNTER, prediction)\n",
    "    \n",
    "else:\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac15d5-07d5-4942-a3a5-3ca1df310cdf",
   "metadata": {},
   "source": [
    "# Depricated Code  "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

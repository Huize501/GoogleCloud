{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Demo Tensorflow and Cloud Machine Learning Engine using the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this demo we will be going through the steps of building a Machine Learning Pipeline using Tensorflow and Cloud Machine Learning Engine. This demo uses and explains best practices that can be used to build a ML pipeline using GCP and Tensorflow. The end product is a ML pipeline that creates an API that can be used for online inference. \n",
    "\n",
    "This notebook is intended to be run on Google Cloud [Datalab](https://cloud.google.com/datalab/docs/quickstarts)\n",
    "\n",
    "Datalab will have the required libraries installed by default for this code to work. If you choose to run this code outside of Datalab you may run in to version and dependency issues which you will need to resolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "from tensorflow.contrib.learn.python.learn.utils import (saved_model_export_utils)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tensorflow APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Tensorflow is a hierarchical framework. The further down the hierarchy you go, the more flexibility you have, but that more code you have to write. A best practice is start at the highest level of abstraction. Datasets and Estimators are two key TensorFlow higher level API's that are used a lot for ML projects:\n",
    "\n",
    "- Datasets: Best practice way of creating input pipelines. \n",
    "- Estimators: A high-level API to create TensorFlow models. Estimators include canned models (out of the box) and custom estimators.\n",
    "\n",
    "Below you the TensorFlow architecture including the dataset API an Estimators. Combined, they offer an easy way to create TensorFlow models.\n",
    "\n",
    "![title](https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png)\n",
    "\n",
    "For this tutorial we will be operating at the highest level of Tensorflow abstraction, using the Estimator API.\n",
    "\n",
    "Steps of this demo:\n",
    "- 1: Get raw data\n",
    "- 2: Write Tensorflow Code\n",
    "    - Define Meta data and Feature Columns\n",
    "    - Define Input Function and Eval Function\n",
    "    - Define Serving Function\n",
    "    - Initialize Estimator\n",
    "    - Define Train and Eval Function\n",
    "- 3: Package Code\n",
    "- 4: Train on Google Cloud using Cloud Machine Learning Engine\n",
    "- 5: Inspect Results using Tensorboard\n",
    "- 6: Deploy Model using Cloug Machine Learning Engine\n",
    "- 7: Get Predictions using the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1) Get raw data (csv) \n",
    "For this demo we are using the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). Which is a famous Machine Learning dataset used for benchmarking and demo's. Be aware that it's a really small dataset and it's only used here to demonstrate how to build a ML pipeline using Tensorflow and Cloud Machine Learning Engine. The trained model categorizes Iris flowers based on four botanical features (sepal length, sepal width, petal length, and petal width). So, during inference, you can provide values for those four features and the model will predict that the flower is one of the following three beautiful variants:\n",
    "\n",
    "![title](https://www.tensorflow.org/images/iris_three_species.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The CSV files are available on GCS and can be downloaded here: [Iris Train](https://storage.googleapis.com/erwinh-ml-demo/models/trainer_estimator_iris/data/iris_training.csv) & [Iris Test](https://storage.googleapis.com/erwinh-ml-demo/models/trainer_estimator_iris/data/iris_test.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Path to training and test data in bucket\n",
    "train_path = 'gs://erwinh-ml-demo/models/trainer_estimator_iris/data/iris_training.csv'\n",
    "test_path = 'gs://erwinh-ml-demo/models/trainer_estimator_iris/data/iris_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2) Write Tensorflow Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2A) Setup Meta Data and Feature Columns\n",
    "Feature columns are your Estimator's data \"interface.\" They tell the estimator in what format they should expect data and how to interpret it (is it one-hot? sparse? dense? continous?). You can read more in the Tensorflow documentation on [Tensorflow Feature Columns](https://www.tensorflow.org/api_docs/python/tf/feature_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we define the feature names + label. In this case we use all features. \n",
    "FEATURE_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth']\n",
    "LABEL = ['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set the data format that can be expected\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in FEATURE_NAMES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2B) Define Input and Evaluation Function\n",
    "Now that you have an estimator and it knows what type of data to expect and how to interpret, you need to actually pass the data to it! This is the job of the input function.\n",
    "\n",
    "The input function returns a (features, label) tuple\n",
    "\n",
    "features: A python dictionary. Each key is a feature column name and its value is the tensor containing the data for that Feature\n",
    "label: A Tensor containing the label column\n",
    "\n",
    "In this case we have a separate function that will take care of decoding the CSV file and reading in the data that is used by the Input Function. On the Tensorflow [Tensorflow Feature Columns](https://www.tensorflow.org/programmers_guide/datasets) you can read more information on some of the best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function takes care of decoding the csv file\n",
    "\n",
    "def decode_csv(line):\n",
    "    parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]]) # parsed line + defaults\n",
    "    label = parsed_line[-1]  # Last element is the label\n",
    "    del parsed_line[-1]  # Delete label\n",
    "    features = parsed_line  # Everything but last element are features\n",
    "    d = dict(zip(FEATURE_NAMES, features)), label \n",
    "    \n",
    "    return d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our input function that will use the tf.data api. The [tf.data.TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) will create a dataset comprising lines from a textfile (csv in this case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we define our input function\n",
    "\n",
    "def generate_input_fn(data_set):\n",
    "    def input_fn(repeat_count=100, shuffle_count=500):\n",
    "       dataset = (tf.data.TextLineDataset(data_set)  # Read text file\n",
    "         .skip(1)  # Skip header row\n",
    "         .map(decode_csv)  # uses decode_csv function\n",
    "         .shuffle(shuffle_count) \n",
    "         .repeat(repeat_count) # repeat dataset count times\n",
    "         .batch(30)\n",
    "         .prefetch(90)  # Dataset that prefetches elements from this dataset\n",
    "       )\n",
    "       iterator = dataset.make_one_shot_iterator()\n",
    "       batch_features, batch_labels = iterator.get_next()\n",
    "       return batch_features, batch_labels # Return tuple with features and labels\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this is the input function that we use for evaluation\n",
    "\n",
    "def eval_input_fn(data_set):\n",
    "    def eval_input_fn(repeat_count=1, shuffle_count=1):\n",
    "       dataset = (tf.data.TextLineDataset(data_set)  # Read text file\n",
    "         .skip(1)  # Skip header row\n",
    "         .map(decode_csv, num_parallel_calls=2)  # Decode each line \n",
    "         .shuffle(shuffle_count)  \n",
    "         .repeat(repeat_count)    \n",
    "         .batch(30)\n",
    "         .prefetch(90)  # Maximum number of elements that will be buffered when prefetching\n",
    "       )\n",
    "       iterator = dataset.make_one_shot_iterator()\n",
    "       batch_features, batch_labels = iterator.get_next()\n",
    "       return batch_features, batch_labels\n",
    "    return eval_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2C) Create input function for serving \n",
    "We need to define a serving input function which will be used to read inputs at prediction time.\n",
    "\n",
    "We create this input function since we may be receiving data in a different format during serving. The serving input function performs transformation necessary to get the data provided at prediction time into the format compatible with the Estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is the input_fn we use for serving\n",
    "\n",
    "def serving_input_fn ():\n",
    "  #feature_placeholders are what the caller of the predict() method will have to provide\n",
    "  feature_placeholders = {\n",
    "      column.name: tf.placeholder(column.dtype, [None])\n",
    "      for column in feature_columns\n",
    "  }\n",
    "  \n",
    "  #features are what we actually pass to the estimator\n",
    "  features = {\n",
    "    # Inputs are rank 1 so that we can provide scalars to the server\n",
    "    # but Estimator expects rank 2, so we expand dimension\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "    features, feature_placeholders\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2D) Initialize Estimator: DNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this example we use the [Estimator API](https://www.tensorflow.org/programmers_guide/estimators). An Estimator is what implements the training, eval and prediction loops. The Estimator uses these methods:\n",
    "\n",
    "- fit(): training\n",
    "- eval(): evaluation\n",
    "- predict(): prediction\n",
    "- export_savedmodel(): writing model state to disk\n",
    "\n",
    "Tensorflow has several canned estimator that already implement these methods (DNNClassifier, LogisticClassifier etc..) or you can implement a [custom Estimator](https://www.tensorflow.org/get_started/custom_estimators). For simplicity we will use a canned [Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier).\n",
    "\n",
    "Notice we wrap the estimator with a function. This is to allow us to specify the 'output_dir' at runtime, instead of having to hardcode it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we specify some of the meta data for the model. We also have a temp directory\n",
    "\n",
    "number_hidden_units =[256, 128] # Hidden layers + neuron's\n",
    "number_classes = 3 # Number of classes\n",
    "temp_direct = \"output_dir/iris_%s\" % (int(time.time())) # Temp directory for training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function wil initialize the DNN Classifier\n",
    "\n",
    "def generate_estimator(output_dir):\n",
    "  return tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                    hidden_units=number_hidden_units,\n",
    "                                    model_dir=output_dir,\n",
    "                                    n_classes=number_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2E) Define Train and Eval Function\n",
    "For training and evaluation we are using [tf.estimator.train_and_evaluate()](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate)\n",
    "\n",
    "This function is important because it provides consistent behavior across local and distributed environments. The function will take care of parallelizing the computation graph across these devices (GPU & CPU)!\n",
    "\n",
    "The tran_and_evaluate() function takes three arguments:\n",
    "\n",
    "- estimator: mentioned earlier\n",
    "- train_spec: specifies the training input function\n",
    "- eval_spec: specifies the eval input function, and also the 'exporter' which uses our serving_input_fn for serving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "                input_fn=generate_input_fn(train_path),\n",
    "                max_steps=1000)\n",
    "\n",
    "exporter = tf.estimator.LatestExporter('Servo', serving_input_fn)\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "            input_fn=eval_input_fn(test_path),\n",
    "            steps=10,\n",
    "            exporters=exporter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Test Run DNN Classifier\n",
    "Now its time to take our classifier for a spin and train it locally. It's a best practice to first run locally on a small dataset to check for errors. First do some housekeeping and delete previous checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf output_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe3f9c6fbd0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output_dir/iris_1528992112', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into output_dir/iris_1528992112/model.ckpt.\n",
      "INFO:tensorflow:loss = 30.230007, step = 1\n",
      "INFO:tensorflow:global_step/sec: 426.392\n",
      "INFO:tensorflow:loss = 3.6556778, step = 101 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.064\n",
      "INFO:tensorflow:loss = 4.136495, step = 201 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.744\n",
      "INFO:tensorflow:loss = 3.5906549, step = 301 (0.228 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 397 into output_dir/iris_1528992112/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.0300255.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-14-16:05:26\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-397\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-14-16:05:27\n",
      "INFO:tensorflow:Saving dict for global step 397: accuracy = 0.9655172, average_loss = 0.09915771, global_step = 397, loss = 2.8755736\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-397\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: output_dir/iris_1528992112/export/Servo/temp-1528992327/saved_model.pb\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-397\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 398 into output_dir/iris_1528992112/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.157688, step = 398\n",
      "INFO:tensorflow:global_step/sec: 440.067\n",
      "INFO:tensorflow:loss = 3.083871, step = 498 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.376\n",
      "INFO:tensorflow:loss = 1.8324497, step = 598 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.154\n",
      "INFO:tensorflow:loss = 1.346789, step = 698 (0.230 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 794 into output_dir/iris_1528992112/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.13970272.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-14-16:05:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-794\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-14-16:05:30\n",
      "INFO:tensorflow:Saving dict for global step 794: accuracy = 0.9655172, average_loss = 0.0650188, global_step = 794, loss = 1.8855453\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-794\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: output_dir/iris_1528992112/export/Servo/temp-1528992330/saved_model.pb\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-794\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 795 into output_dir/iris_1528992112/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2678826, step = 795\n",
      "INFO:tensorflow:global_step/sec: 423.784\n",
      "INFO:tensorflow:loss = 0.49924612, step = 895 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.765\n",
      "INFO:tensorflow:loss = 6.2983637, step = 995 (0.235 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into output_dir/iris_1528992112/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.0868773.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-14-16:05:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-14-16:05:32\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9655172, average_loss = 0.11590948, global_step = 1000, loss = 3.3613749\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'SepalLength': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'PetalLength': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'PetalWidth': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'SepalWidth': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from output_dir/iris_1528992112/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: output_dir/iris_1528992112/export/Servo/temp-1528992332/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# This will train the estimator in the local environment. Accuracy can be found at the end. \n",
    "\n",
    "tf.estimator.train_and_evaluate(generate_estimator(temp_direct), train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3) Package Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we need to do some housekeeping again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have all the Tensorflow code needed and it runs! To scale the training and inference using Cloud Machine Learning Engine we need to package the code into some python files and make some changes:\n",
    "\n",
    "- Add some boilerplate code to parse the command line arguments required for gcloud.\n",
    "- Use the learn_runner.run() function to run the experiment\n",
    "- We also add an empty __init__.py file to the folder. This is just the python convention for identifying modules.\n",
    "\n",
    "There is a great [blogpost](https://towardsdatascience.com/how-to-train-machine-learning-models-in-the-cloud-using-cloud-ml-engine-3f0d935294b3) with more details on how to package Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir trainer\n",
    "touch trainer/__init__.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "# Import libraries\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "from tensorflow.contrib.learn.python.learn.utils import (saved_model_export_utils)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Path to training and test data in bucket\n",
    "train_path = 'gs://erwinh-ml-demo/models/trainer_estimator_iris/data/iris_training.csv'\n",
    "test_path = 'gs://erwinh-ml-demo/models/trainer_estimator_iris/data/iris_test.csv'\n",
    "\n",
    "# Specify meta data\n",
    "FEATURE_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth']\n",
    "LABEL = ['Species']\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in FEATURE_NAMES]\n",
    "\n",
    "# Model hyperparameter + number of classes\n",
    "number_hidden_units =[512, 256, 128] \n",
    "number_classes = 3\n",
    "\n",
    "# Generate DNN Classifier\n",
    "def generate_estimator(output_dir):\n",
    "  return tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                    hidden_units=number_hidden_units,\n",
    "                                    model_dir=output_dir,\n",
    "                                    n_classes=number_classes)\n",
    "\n",
    "# Function to decode csv: features & labels\n",
    "def decode_csv(line):\n",
    "    parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "    label = parsed_line[-1]  # Last element is the label\n",
    "    del parsed_line[-1]  # Delete last element\n",
    "    features = parsed_line  # Everything but last elements are the features\n",
    "    d = dict(zip(FEATURE_NAMES, features)), label\n",
    "    \n",
    "    return d\n",
    "\n",
    "# Input function that uses the decode csv to return features and labels\n",
    "def generate_input_fn(data_set):\n",
    "    def input_fn(repeat_count=100, shuffle_count=500):\n",
    "       dataset = (tf.data.TextLineDataset(data_set)  # Read text file\n",
    "         .skip(1)  # Skip header row\n",
    "         .map(decode_csv)  # Decode each line \n",
    "         .shuffle(shuffle_count)  # Randomize elems (1 == no operation)\n",
    "         .repeat(repeat_count)    # Repeats dataset this # times\n",
    "         .batch(32)\n",
    "         .prefetch(150)  # Maximum number of elements that will be buffered when prefetching\n",
    "       )\n",
    "       iterator = dataset.make_one_shot_iterator()\n",
    "       batch_features, batch_labels = iterator.get_next()\n",
    "       return batch_features, batch_labels\n",
    "    return input_fn\n",
    "\n",
    "# Input function used for evaulation data\n",
    "def eval_input_fn(data_set):\n",
    "    def eval_input_fn(repeat_count=1, shuffle_count=1):\n",
    "       dataset = (tf.data.TextLineDataset(data_set)  # Read text file\n",
    "         .skip(1)  # Skip header row\n",
    "         .map(decode_csv, num_parallel_calls=2)  # Decode each line \n",
    "         .shuffle(shuffle_count)  # Randomize elems (1 == no operation)\n",
    "         .repeat(repeat_count)    # Repeats dataset this # times\n",
    "         .batch(32)\n",
    "         .prefetch(150)  # Maximum number of elements that will be buffered when prefetching\n",
    "       )\n",
    "       iterator = dataset.make_one_shot_iterator()\n",
    "       batch_features, batch_labels = iterator.get_next()\n",
    "       return batch_features, batch_labels\n",
    "    return eval_input_fn\n",
    "\n",
    "# Serving input function that will be used for online inference\n",
    "def  serving_input_fn ():\n",
    "  #feature_placeholders are what the caller of the predict() method will have to provide\n",
    "  feature_placeholders = {\n",
    "      column.name: tf.placeholder(column.dtype, [None])\n",
    "      for column in feature_columns\n",
    "  }\n",
    "  \n",
    "  #features are what we actually pass to the estimator\n",
    "  features = {\n",
    "    # Inputs are rank 1 so that we can provide scalars to the server\n",
    "    # but Estimator expects rank 2, so we expand dimension\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "    features, feature_placeholders\n",
    "  )\n",
    "\n",
    "# Setup Train Spec, Evaluation Spec and Exporter.\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "                input_fn=generate_input_fn(train_path),\n",
    "                max_steps=2000)\n",
    "\n",
    "exporter = tf.estimator.LatestExporter('Servo', serving_input_fn)\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "            input_fn=eval_input_fn(test_path),\n",
    "            steps=10,\n",
    "            exporters=exporter)\n",
    "\n",
    "######START CLOUD ML ENGINE BOILERPLATE######\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='this model ignores this field, but it is required by gcloud',\n",
    "        default='junk'\n",
    "    )\n",
    "  args = parser.parse_args()\n",
    "  arguments = args.__dict__\n",
    "  output_dir = arguments.pop('output_dir')\n",
    "  \n",
    "  ######END CLOUD ML ENGINE BOILERPLATE######\n",
    "\n",
    "  #initiate training job\n",
    "  tf.estimator.train_and_evaluate(generate_estimator(output_dir), train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4) Train on Google Cloud using Cloud Machine Learning Engine\n",
    "\n",
    "Now that our code is packaged we can invoke it using the gcloud command line tool to run a job using Cloud Machine Learning Engine.\n",
    "\n",
    "Note: Since our dataset is so small and our model is simple the overhead of provisioning the cluster is longer than the actual training time. Accordingly you'll notice the single VM cloud training takes longer than the local training, and the distributed cloud training takes longer than single VM cloud. For larger datasets and more complex models this will reverse\n",
    "\n",
    "We'll need to create environment variables for our project name and Google Cloud Storage Bucket. Keep your data (bucket) and Cloud Machine Learning Engine environment in the same region. \n",
    "\n",
    "You need to change some of the variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change these variables to you own\n",
    "\n",
    "GCS_BUCKET = 'gs://erwinh-ml-demo/models/output_iris' # CHANGE THIS TO YOUR BUCKET \n",
    "PROJECT = 'erwinh-mldemo' # CHANGE THIS TO YOUR PROJECT ID\n",
    "REGION = 'us-central1' # OPTIONALLY CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GCS_BUCKET'] = GCS_BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4A) Train Local\n",
    "You can also use the gcloud command to train your model locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=trainer \\\n",
    "   -- \\\n",
    "   --output_dir='./output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4B) Train in the Cloud on one ML unit: n1-standard-4 machine\n",
    "Here we are training in the cloud. We add a unique jobnime using time and date. Then we go and submit the job to Cloud Machine Learning Engine using the gcloud ml-eninge command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: iris_180614_162024\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [iris_180614_162024] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe iris_180614_162024\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs iris_180614_162024\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "JOBNAME=iris_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME/ \\\n",
    "   --runtime-version 1.8 \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4C) Train in the Cloud on STANDARD_1\n",
    "\n",
    "This will consist of: master: n1-highcpu-8, workers: n1-highcpu-8, parameter servers: n1-standard-4. \n",
    "\n",
    "Because we are using the TF Estimators interface distributed training is easy. We are changing the scale-tier that we want to use. In the [documentation](https://cloud.google.com/ml-engine/pricing#ml_training_units_by_scale_tier) you can find more information on the different tier options. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: iris_180614_162137\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [iris_180614_162137] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe iris_180614_162137\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs iris_180614_162137\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "JOBNAME=iris_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --runtime-version 1.8 \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4D) Train on GPU's\n",
    "The \"BASIC_GPU\" corresponds to one Tesla K80 at the time of this writing, hardware subject to change. Bew aware that it takes some time to spin up the recources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: iris_180611_164636\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [iris_180611_164636] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe iris_180611_164636\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs iris_180611_164636\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "JOBNAME=iris_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --runtime-version 1.8 \\\n",
    "   --scale-tier=BASIC_GPU \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4F) Run on custom Tier \n",
    "It's possible to create a custom tier. In the [documentation](https://cloud.google.com/ml-engine/docs/pricing) you can read about the machines you can select and the pricing. First we create a yaml file to specify the:\n",
    "\n",
    "- Tier type\n",
    "- Master and worker type\n",
    "- WorkerCount\n",
    "\n",
    "I choose for the comples_model_s type in the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: complex_model_s\n",
    "  workerType: complex_model_s\n",
    "  workerCount: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After creating the config file we can add it to the gcloud command and run the job. This will take a bit more time to run since it needs to spin up the specific machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: iris_180611_165912\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [iris_180611_165912] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe iris_180611_165912\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs iris_180611_165912\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "JOBNAME=iris_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --runtime-version 1.8 \\\n",
    "   --config config.yaml \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5) Inspect results using Tensorboard\n",
    "\n",
    "Tensorboard is a utility that allows you to visualize your results. To make it work you need to change the path to where you have written your output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: cd: gs://erwinh-ml-demo/models/output_iris//output: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 9919. Click <a href=\"/_proxy/34387/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9919"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('output') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code will stop Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6) Deploy the model as an API using Cloud Machine Learning Engine\n",
    "\n",
    "Here you need to change and check some of the variables below. Please check:\n",
    "\n",
    "- MODEL_NAME: Give this a name (you can leave it like this). Make sure it unique.  \n",
    "- MODEL_VERSION: Specify a version. \n",
    "- MODEL_LOCATION: Change this to where your model is saved. You can find it in the output/export/servo directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/erwinh-mldemo/models/Iris_Model].\n",
      "Creating version (this might take a few minutes)......\n",
      ".................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"Iris_Model\"\n",
    "MODEL_VERSION=\"v2\"\n",
    "MODEL_LOCATION='gs://erwinh-ml-demo/models/output_iris/iris_180611_103147/output/export/Servo/1528713201/' \n",
    "\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME} #Uncomment to overwrite existing version\n",
    "#gcloud ml-engine models delete ${MODEL_NAME} #Uncomment to overwrite existing model\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --staging-bucket='gs://huize501'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 7) Get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cloud Machine Learning Engine supports two ways of doing predictions: Online (API) and Batch. \n",
    "\n",
    "When to use what:\n",
    "- Online prediction is more appropriate for latency sensitive requests as results are returned quickly and synchronously.\n",
    "- Batch prediction is more appropriate for large prediction requests that you only need to run a few times a day.\n",
    "\n",
    "The prediction services expects prediction requests in standard JSON format so first we will create a JSON file with a couple of housing records. So we will create a JSON file that we can use to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting records.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile records.json\n",
    "{\"SepalLength\": 6.9,\"SepalWidth\": 3.1,\"PetalLength\": 5.4,\"PetalWidth\": 2.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can pass the JSON file to the API using gcloud. The results are returned immediatley.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS_IDS  CLASSES  LOGITS                                                         PROBABILITIES\r\n",
      "[2]        [u'2']   [-20.320819854736328, -13.325811386108398, 33.51441192626953]  [4.16539226112411e-24, 4.5451547097935835e-21, 1.0]\r\n",
      "\r\n",
      "\r\n",
      "Updates are available for some Cloud SDK components.  To install them,\r\n",
      "please run:\r\n",
      "  $ gcloud components update\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine predict --model Iris_Model --json-instances records.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What Have we seen in this Notebook?\n",
    "\n",
    "- 1: Get raw data\n",
    "- 2: Write Tensorflow Code\n",
    "- 3: Package Code\n",
    "- 4: Train on Google Cloud using Cloud Machine Learning Engine\n",
    "- 5: Inspect Results using Tensorboard\n",
    "- 6: Deploy Model using Cloug Machine Learning Engine\n",
    "- 7: Get Predictions using the API\n",
    "\n",
    "What we will safe for the next demo:\n",
    "- Feature engineering using Dataflow \n",
    "- Take data from BigQuery\n",
    "- Hyperparameter tuning\n",
    "- Training model using TPU's\n",
    "\n",
    "This notebook is build to give you an introduction in to building Tensorflow models using Datasets and Estimators that can be trained and deployed using Cloud Machine Learning Engine. If you want to learn more it's best to have a look at our Online How Google Does Machine Learning [Course](https://www.coursera.org/learn/google-machine-learning) available on Coursera. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
